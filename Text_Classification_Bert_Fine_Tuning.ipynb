{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project was part of my Applied Machine Learning CSCI-B 565 assignment at Indiana University."
      ],
      "metadata": {
        "id": "zOvy4i-xfp88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6lZcdrwWK3n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhLzNdi3fgIt"
      },
      "source": [
        "I have created my own dataset containing at least 1000 words in total and at least two categories with at least 100 examples per category.\n",
        "\n",
        "I have web-scrapped quotes from 'Star Wars' and 'Friends' Series using selenium and BeautifulSoup. For Classification: 0 is Star wars and 1 is Friends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg5dsRnhWc7E",
        "outputId": "386f3544-b41d-4773-f04c-7dc4e9dc98ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AvxQPVe9zwFpyA74gI2_nnvbNVQ0X5Ci\n",
            "To: /geode2/home/u030/gchaudh/Carbonate/Desktop/hw4_q2/star_wars.txt\n",
            "100%|██████████| 5.77k/5.77k [00:00<00:00, 3.88MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oohK6Yplzh43d-qmu7LRbhF789CnKhJT\n",
            "To: /geode2/home/u030/gchaudh/Carbonate/Desktop/hw4_q2/friends.txt\n",
            "100%|██████████| 7.37k/7.37k [00:00<00:00, 13.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Star Wars quotes\n",
        "star_wars_url = '1AvxQPVe9zwFpyA74gI2_nnvbNVQ0X5Ci'\n",
        "star_wars_download_url = f\"https://drive.google.com/uc?id={star_wars_url}\"\n",
        "gdown.download(star_wars_download_url, 'star_wars.txt', quiet=False)\n",
        "star_wars = pd.read_fwf('star_wars.txt',header=None, names=['Quotes'],encoding='utf-8')\n",
        "\n",
        "# Download Friends quotes\n",
        "friends_url = '1oohK6Yplzh43d-qmu7LRbhF789CnKhJT'\n",
        "friends_download_url = f\"https://drive.google.com/uc?id={friends_url}\"\n",
        "gdown.download(friends_download_url, 'friends.txt', quiet=False)\n",
        "# friends = pd.read_fwf('friends.txt', encoding='utf-8')\n",
        "\n",
        "friends = pd.read_csv('friends.txt', delimiter='\\t', header=None, names=['Quotes'])\n",
        "\n",
        "# Add labels\n",
        "star_wars['Label'] = 0\n",
        "friends['Label'] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEvl-jmRaW6_",
        "outputId": "fef97be2-70ab-4a2f-b510-e0f633df0671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((100, 2), (100, 2))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "star_wars.shape, friends.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8SFF0LMZKY6"
      },
      "outputs": [],
      "source": [
        "quotes = pd.concat([star_wars, friends], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou2O_vk6fgIx",
        "outputId": "4f6c3d40-bfbd-4d1a-8175-e48d3da95fe7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Quotes</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>“Try not. Do or do not. There is no try.”</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“Your eyes can deceive you; don’t trust them.”</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>“Luminous beings we are, not this crude matter.”</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“Who’s the more foolish: the fool or the fool ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“Your focus determines your reality.”</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Quotes  Label\n",
              "0          “Try not. Do or do not. There is no try.”      0\n",
              "1     “Your eyes can deceive you; don’t trust them.”      0\n",
              "2   “Luminous beings we are, not this crude matter.”      0\n",
              "3  “Who’s the more foolish: the fool or the fool ...      0\n",
              "4              “Your focus determines your reality.”      0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quotes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP33dulEfgIx"
      },
      "source": [
        "We decided to perform some preliminary basic data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQOLHumbYLR"
      },
      "outputs": [],
      "source": [
        "# Removing quotations\n",
        "quotes['Quotes'] = quotes['Quotes'].str.replace('\"', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82FNNFYafgIx"
      },
      "source": [
        "Split the dataset into training (at least 160examples) and test (at least 40 examples) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71vlisnvbgW1",
        "outputId": "865b259f-a946-4a10-f15c-6ba8f13fa4c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 160\n",
            "Test set size: 40\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training 160 and test 40\n",
        "train, test = train_test_split(quotes, test_size=0.2, random_state=42,stratify=quotes['Label'])\n",
        "\n",
        "# # Ensure that both sets have at least the specified number of examples\n",
        "# while len(train) < 160 or len(test) < 40:\n",
        "#     quotes = quotes.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "#     train, test = train_test_split(quotes, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", len(train))\n",
        "print(\"Test set size:\", len(test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss8yLprWfgIy"
      },
      "outputs": [],
      "source": [
        "X_train = train.drop(columns='Label')\n",
        "y_train = train['Label']\n",
        "X_test = test.drop(columns='Label')\n",
        "y_test = test['Label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92B1pWeUfgIy"
      },
      "source": [
        "Fine tuning a pretrained language model capable of generating text (e.g., GPT) that you can take from the Hugging Face Transformers library with the dataset your created (this tutorial could be very helpful: https://huggingface.co/docs/transformers/training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8188vzY1fgIy"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYG5oEsJfgIy"
      },
      "source": [
        "Tokenizing data and also adding padding and truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvxLcvO9fgIz"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2gfjCexfgIz"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = tokenizer(X_train['Quotes'].tolist(),return_tensors='np',padding=True, truncation=True,max_length=512, return_attention_mask=True)\n",
        "X_test_tokenized = tokenizer(X_test['Quotes'].tolist(),return_tensors='np',padding=True, truncation=True,max_length=512, return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A54MXKAWfgIz"
      },
      "outputs": [],
      "source": [
        "y_train_array = np.array(y_train)\n",
        "y_test_array = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jn0-qv2fgIz",
        "outputId": "b63edd76-cc4b-45d1-df82-882316317bcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 18:08:34.500020: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /N/soft/rhel7/pcre2/10.34/lib:/N/soft/rhel7/gcc/9.3.0/lib64:/N/soft/rhel7/gcc/9.3.0/lib:/N/soft/rhel7/java/11.0.2/lib/server:/N/soft/rhel7/curl/intel/7.54.0/lib:/N/soft/rhel7/python/gnu/3.10.5/lib:/N/soft/rhel7/openmpi/gnu/4.1.4/lib:/N/soft/rhel7/libpng/1.2.57/lib:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/compiler/lib/intel64:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/ipp/lib/intel64:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/compiler/lib/intel64_lin:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64_lin:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/tbb/lib/intel64/gcc4.7:/N/soft/rhel7/intel/19.5/debugger_2019/iga/lib:/N/soft/rhel7/intel/19.5/debugger_2019/libipt/intel64/lib:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/daal/lib/intel64_lin:/opt/thinlinc/lib64:/opt/thinlinc/lib\n",
            "2023-12-11 18:08:34.585473: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-12-11 18:08:34.585589: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (i28.red.uits.iu.edu): /proc/driver/nvidia/version does not exist\n",
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL_9sRp6fgIz"
      },
      "outputs": [],
      "source": [
        "# Creating tensorflow datasets\n",
        "import tensorflow as tf\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train_array)).batch(64)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test_array)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5z7H7lBfgIz"
      },
      "outputs": [],
      "source": [
        "#Compiling and training the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MiJxaJffgI0",
        "outputId": "71213578-dfe0-4477-ecf4-fb1a8d9f0855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3/3 [==============================] - 20s 6s/step - loss: 0.6440 - accuracy: 0.6313\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 20s 6s/step - loss: 0.6036 - accuracy: 0.6875\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 20s 6s/step - loss: 0.5490 - accuracy: 0.7688\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2105cab460>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOGw24jHfgI0",
        "outputId": "3ae0e323-a287-432d-c893-3b262d91a384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 5s 5s/step - loss: 0.6027 - accuracy: 0.7000\n",
            "Test Loss: 0.6027485132217407\n",
            "Test Accuracy: 0.699999988079071\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSb7dYeifgI0"
      },
      "source": [
        "We want to see if it is possible to get higher accuracy by changing the parameters like learning rate, batch_size and epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gex3xRifgI0"
      },
      "outputs": [],
      "source": [
        "# Creating a function to test different parameters\n",
        "def fine_tune_bert(epochs, learning_rate, batch_size=64):\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train_tokenized), y_train_array)).batch(batch_size)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test_tokenized), y_test_array)).batch(batch_size)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "    model.fit(train_dataset, epochs=epochs)\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jhiByYEfgI0",
        "outputId": "ce7d8724-08c3-42fb-b0e7-5a91938ca27f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "5/5 [==============================] - 42s 4s/step - loss: 0.5600 - accuracy: 0.7375\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.5104 - accuracy: 0.7812\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.4190 - accuracy: 0.8375\n",
            "2/2 [==============================] - 4s 247ms/step - loss: 0.5371 - accuracy: 0.7750\n",
            "Test Loss: 0.5370607376098633\n",
            "Test Accuracy: 0.7749999761581421\n"
          ]
        }
      ],
      "source": [
        "fine_tune_bert(epochs=3, learning_rate=2e-5, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ue_tZHGfgI0",
        "outputId": "801361a4-4a21-427d-fef7-9671f6bfe0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3/3 [==============================] - 40s 6s/step - loss: 0.4992 - accuracy: 0.8250\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 19s 6s/step - loss: 0.4356 - accuracy: 0.8750\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 20s 6s/step - loss: 0.3911 - accuracy: 0.8813\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 24s 6s/step - loss: 0.3686 - accuracy: 0.8875\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 19s 6s/step - loss: 0.3158 - accuracy: 0.9062\n",
            "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_test_function.<locals>.test_function at 0x7f20ff29f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.8103 - accuracy: 0.6750\n",
            "Test Loss: 0.8103048205375671\n",
            "Test Accuracy: 0.675000011920929\n"
          ]
        }
      ],
      "source": [
        "fine_tune_bert(epochs=5, learning_rate=1e-5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y-C-alafgI1",
        "outputId": "5680bcf1-e4d7-45d0-868d-68c08d4f08a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5/5 [==============================] - 43s 4s/step - loss: 0.3664 - accuracy: 0.8750\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.3868 - accuracy: 0.8250\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.3121 - accuracy: 0.8875\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.2782 - accuracy: 0.9187\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 20s 4s/step - loss: 0.2253 - accuracy: 0.9563\n",
            "2/2 [==============================] - 6s 260ms/step - loss: 0.4950 - accuracy: 0.8000\n",
            "Test Loss: 0.49499520659446716\n",
            "Test Accuracy: 0.800000011920929\n"
          ]
        }
      ],
      "source": [
        "fine_tune_bert(epochs=5, learning_rate=1e-5, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f4nLYCXfgI1"
      },
      "source": [
        "## Conclusion-\n",
        "The highest accuracy we got on the test dataset was 80% with 5 epochs, 1e-5 learning rate and batch size as 32.\n",
        "Few ways to improve the model and hence is the accuracy is -\n",
        "- Increasing the number of samples in the dataset. 100 samples is less for the model to learn the representation of both the classes.\n",
        "- Using more techniques to fine tune the model like transfer learning or perhaps experimenting with different pretrained models like distilbert, or gpt3. Also experimenting with larger sized pretrained models like bert-lg\n",
        "- Another way is experimenting more with the hyperparameters.\n",
        "- Add more layers and use regularization and dropout in the experiments\n",
        "- We could also try preprocessing the text like removing stopwords, punctuation to see if the accuracy improves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJQVRhKCfgI1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}